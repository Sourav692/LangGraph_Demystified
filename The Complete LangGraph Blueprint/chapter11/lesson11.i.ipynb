{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List, TypedDict\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "#1. Index 3 websites by adding them to a vector DB\n",
    "urls = [\n",
    "    \"https://github.com/facebookresearch/faiss\",\n",
    "    \"https://github.com/facebookresearch/faiss/wiki\",\n",
    "    \"https://github.com/facebookresearch/faiss/wiki/Faiss-indexes\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#2. Prepare the RAG chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "rag_chain = (\n",
    "    prompt    | model    | StrOutputParser()\n",
    ")\n",
    "\n",
    "#3. define the graph\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation        \n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "#4. Retrieve node\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "#5. Generate node\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "#6. Define the workflow\n",
    "def create_workflow():\n",
    "    workflow = StateGraph(GraphState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"retrieve\", retrieve)\n",
    "    workflow.add_node(\"generate\", generate)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", END)  \n",
    "    \n",
    "    return workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "#7. Run the workflow\n",
    "\n",
    "async def run_workflow():\n",
    "    app = create_workflow()\n",
    "    config = {\n",
    "        \"configurable\": {\"thread_id\": \"1\"},\n",
    "        \"recursion_limit\": 50\n",
    "    }\n",
    "    \n",
    "    inputs = {\"question\": f\"What are flat indexs?\"}\n",
    "    \n",
    "    try:\n",
    "        async for event in app.astream(inputs, config=config, stream_mode=\"values\"):\n",
    "            if \"error\" in event:\n",
    "                print(f\"Error: {event['error']}\")\n",
    "                break\n",
    "            print(event)\n",
    "    except Exception as e:\n",
    "        print(f\"Workflow execution failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_workflow())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
