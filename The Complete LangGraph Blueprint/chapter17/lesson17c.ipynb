{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import TypedDict\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "# Define the agent's state\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    response: str\n",
    "\n",
    "# Initialize the AI tool (e.g., OpenAI API)\n",
    "llm_tool = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", streaming=True)\n",
    "\n",
    "# Define the node that processes user queries\n",
    "def handle_query(state: AgentState) -> AgentState:\n",
    "    user_message = HumanMessage(content=state['query'])\n",
    "    ai_response = llm_tool.invoke([user_message])\n",
    "    state['response'] = ai_response.content\n",
    "    return state\n",
    "\n",
    "# Build the LangGraph workflow for the agent\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"handle_query\", handle_query)\n",
    "builder.add_edge(START, \"handle_query\")\n",
    "builder.add_edge(\"handle_query\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://localhost:3000\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.post(\"/api/research\")\n",
    "async def research_query(request: QueryRequest):\n",
    "    try:        \n",
    "        \n",
    "        initial_state = {\"query\": request.query, \"response\": \"\"}\n",
    "        result = graph.invoke(initial_state)\n",
    "        return {\"response\": result['response']}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/research/stream\")\n",
    "async def research_query_stream(request: QueryRequest):\n",
    "    async def event_generator():\n",
    "        try:\n",
    "            initial_state = {\"query\": request.query, \"response\": \"\"}            \n",
    "            async for msg,metadata in graph.astream(initial_state, stream_mode=\"messages\"):\n",
    "                if msg.content and not isinstance(msg, HumanMessage):\n",
    "                    yield msg.content\n",
    "        except Exception as e:\n",
    "            yield {\"error\": str(e)}\n",
    "    \n",
    "    return StreamingResponse(event_generator(), media_type=\"text/plain\")\n",
    "\n",
    "#RUN THE APP\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
