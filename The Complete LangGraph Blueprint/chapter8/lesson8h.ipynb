{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    ")\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# Define the state for product recommendation\n",
    "class RecommendationState(TypedDict):\n",
    "    user_id: str        # User identifier\n",
    "    preference: str     # User's current preference (e.g., genre, category)\n",
    "    reasoning: str      # Reasoning process from LLM\n",
    "    recommendation: str # Final product recommendation\n",
    "    memory: dict        # User memory to store preferences\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Tool function: Recommend a product based on the user's preference\n",
    "@tool\n",
    "def recommend_product(preference: str) -> str:\n",
    "    \"\"\"Recommend a product based on the user's preferences.\"\"\"\n",
    "    product_db = {\n",
    "        \"science\": \"I recommend 'A Brief History of Time' by Stephen Hawking.\",\n",
    "        \"technology\": \"I recommend 'The Innovators' by Walter Isaacson.\",\n",
    "        \"fiction\": \"I recommend 'The Alchemist' by Paulo Coelho.\"\n",
    "    }\n",
    "    return product_db.get(preference, \"I recommend exploring our latest products!\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import ToolMessage, SystemMessage\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "tools = [recommend_product]\n",
    "llm = llm.bind_tools(tools)\n",
    "\n",
    "# Tool function: Update the user's memory with the latest preference\n",
    "def update_memory(state: RecommendationState):\n",
    "    # Store the user's preference in the memory\n",
    "    state[\"memory\"][state[\"user_id\"]] = state[\"preference\"]\n",
    "    return state\n",
    "\n",
    "\n",
    "# Tool node to handle product recommendation\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "def tool_node(state: RecommendationState):\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "# LLM reasoning node to process user input and generate product recommendations\n",
    "def call_model(state: RecommendationState, config: RunnableConfig):\n",
    "    system_prompt = SystemMessage(\n",
    "        content=f\"You are a helpful assistant for recommending a product based on the user's preference.\"\n",
    "    )\n",
    "    response = llm.invoke([system_prompt]+ state[\"messages\"] + [(\"user\", state[\"preference\"])], config)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Conditional function to determine whether to call the tool or end\n",
    "def should_continue(state: RecommendationState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "     # If there is no tool call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "workflow = StateGraph(RecommendationState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"update_memory\", update_memory)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"update_memory\")\n",
    "workflow.add_edge(\"update_memory\", \"agent\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Initialize the agent's state with the user's preference and memory\n",
    "initial_state = {\"messages\": [(\"user\", \"I'm looking for a book.\")],\"user_id\": \"user1\", \"preference\": \"science\", \"memory\": {}, \"reasoning\": \"\"}\n",
    "\n",
    "# Run the agent\n",
    "result = graph.invoke(initial_state)\n",
    "\n",
    "# Display the final result\n",
    "print(f\"Reasoning: {result['reasoning']}\")\n",
    "print(f\"Product Recommendation: {result['messages'][-1].content}\")\n",
    "print(f\"Updated Memory: {result['memory']}\")\n",
    "\n",
    "# Helper function to print the conversation\n",
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "# Run the agent\n",
    "print_stream(graph.stream(initial_state, stream_mode=\"values\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
