{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Define documents for indexing\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\n",
    "]\n",
    "\n",
    "# Load and split documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Store documents in a vector database (Chroma)\n",
    "vectorstore = Chroma.from_documents(doc_splits, collection_name=\"adaptive-rag\", embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "from typing import List, Literal, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define routing model\n",
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"]\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert at routing a user question to vectorstore or web search.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "question_router = route_prompt | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0).with_structured_output(RouteQuery)\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Evaluate if the document is relevant to the question. Answer 'yes' or 'no'.\"),\n",
    "    (\"human\", \"Document: {document}\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "retrieval_grader = grade_prompt | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0).with_structured_output(GradeDocuments)\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.schema import Document\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "def web_search(state):\n",
    "    search_results = web_search_tool.invoke({\"query\": state[\"question\"]})\n",
    "    web_documents = [Document(page_content=result[\"content\"]) for result in search_results if \"content\" in result]\n",
    "    return {\"documents\": web_documents, \"question\": state[\"question\"]}\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "\n",
    "# Define nodes for query handling\n",
    "def retrieve(state):\n",
    "    documents = retriever.invoke(state[\"question\"])\n",
    "    return {\"documents\": documents, \"question\": state[\"question\"]}\n",
    "\n",
    "def grade_documents(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    filtered_docs = []\n",
    "    web_search_needed = \"No\"\n",
    "    \n",
    "    for doc in documents:\n",
    "        grade = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content}).binary_score\n",
    "        if grade == \"yes\":\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            web_search_needed = \"Yes\"\n",
    "    \n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search_needed}\n",
    "\n",
    "def generate(state):\n",
    "    prompt_template = \"\"\"\n",
    "    Use the following context to answer the question concisely and accurately:\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Define ChatPromptTemplate using the above template\n",
    "    rag_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    # Create the RAG generation chain with LLM and output parsing\n",
    "    rag_chain = (\n",
    "        rag_prompt |\n",
    "        ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) |\n",
    "        StrOutputParser()\n",
    "    )\n",
    "    generation = rag_chain.invoke({\"context\": state[\"documents\"], \"question\": state[\"question\"]})\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "# Route question based on source\n",
    "def route_question(state):\n",
    "    source = question_router.invoke({\"question\": state[\"question\"]}).datasource\n",
    "    return \"web_search\" if source == \"web_search\" else \"retrieve\"\n",
    "\n",
    "# Compile and Run the Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "workflow.add_conditional_edges(START, route_question, {\"web_search\": \"web_search\", \"retrieve\": \"retrieve\"})\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_edge(\"grade_documents\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run with example inputs\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
