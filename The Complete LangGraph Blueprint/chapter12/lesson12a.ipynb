{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, TypedDict\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from display_graph import display_graph\n",
    "\n",
    "# Load and prepare documents\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=doc_splits, collection_name=\"rag-chroma\", embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Set up prompt and model\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the question concisely:\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "rag_chain = (prompt | model | StrOutputParser())\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "\n",
    "# Retrieval Grader setup\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "retrieval_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a grader assessing if a document is relevant to a user's question.\n",
    "Document: {document} \n",
    "Question: {question}\n",
    "Is the document relevant? Answer 'yes' or 'no'.\n",
    "\"\"\")\n",
    "retrieval_grader = retrieval_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(GradeDocuments)\n",
    "\n",
    "# Hallucination Grader setup\n",
    "class GradeHallucinations(BaseModel):\n",
    "    binary_score: str = Field(description=\"Answer is grounded in the documents, 'yes' or 'no'\")\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a grader assessing if an answer is grounded in retrieved documents.\n",
    "Documents: {documents} \n",
    "Answer: {generation}\n",
    "Is the answer grounded in the documents? Answer 'yes' or 'no'.\n",
    "\"\"\")\n",
    "hallucination_grader = hallucination_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Answer Grader setup\n",
    "class GradeAnswer(BaseModel):\n",
    "    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a grader assessing if an answer addresses the user's question.\n",
    "Question: {question} \n",
    "Answer: {generation}\n",
    "Does the answer address the question? Answer 'yes' or 'no'.\n",
    "\"\"\")\n",
    "answer_grader = answer_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(GradeAnswer)\n",
    "\n",
    "# Define LangGraph functions\n",
    "def retrieve(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Grades documents based on relevance to the question.\n",
    "    Only relevant documents are retained in 'relevant_docs'.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    relevant_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        response = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "        if response.binary_score == \"yes\":\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    return {\"documents\": relevant_docs, \"question\": question}\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Decides whether to proceed with generation or transform the query.\n",
    "    \"\"\"\n",
    "    if not state[\"documents\"]:\n",
    "        return \"transform_query\"  # No relevant docs found; rephrase query\n",
    "    return \"generate\"  # Relevant docs found; proceed to generate\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Checks if the generation is grounded in retrieved documents and answers the question.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    # Step 1: Check if the generation is grounded in documents\n",
    "    hallucination_check = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    \n",
    "    if hallucination_check.binary_score == \"no\":\n",
    "        return \"not supported\"  # Regenerate if generation isn't grounded in documents\n",
    "\n",
    "    # Step 2: Check if generation addresses the question\n",
    "    answer_check = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "    return \"useful\" if answer_check.binary_score == \"yes\" else \"not useful\"\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Rephrases the query for improved retrieval if initial attempts do not yield relevant documents.\n",
    "    \"\"\"\n",
    "    transform_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are a question re-writer that converts an input question to a better version optimized for retrieving relevant documents.\n",
    "    Original question: {question} \n",
    "    Please provide a rephrased question.\n",
    "    \"\"\")\n",
    "\n",
    "    question_rewriter = transform_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) | StrOutputParser()\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    # Rephrase the question using LLM\n",
    "    transformed_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"question\": transformed_question, \"documents\": state[\"documents\"]}\n",
    "\n",
    "# Set up the workflow graph\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\"grade_documents\", decide_to_generate, {\"transform_query\": \"transform_query\", \"generate\": \"generate\"})\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\"generate\", grade_generation_v_documents_and_question, {\"not supported\": \"generate\", \"useful\": END, \"not useful\": \"transform_query\"})\n",
    "\n",
    "# Compile the app and run\n",
    "app = workflow.compile()\n",
    "\n",
    "# Display the graph\n",
    "display_graph(app, file_name=os.path.basename(__file__))\n",
    "\n",
    "# Example input\n",
    "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
    "for output in app.stream(inputs):\n",
    "    print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
