{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from arxiv import Client, Search\n",
    "import arxiv\n",
    "from langchain_core.tools import tool\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, List, Tuple, Union, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Enhanced tools with error handling and input validation\n",
    "@tool\n",
    "async def search_research_papers(topic: str) -> str:\n",
    "    \"\"\"Searches for research papers on a given topic using the Arxiv API.\"\"\"\n",
    "    fake_research_paper = \"Langgraph is a graph-based workflow research paper.\"\n",
    "    return fake_research_paper\n",
    "\n",
    "@tool\n",
    "def extract_data(paper_url: str) -> str:\n",
    "    \"\"\"Extracts relevant data from a research paper's URL.\"\"\"\n",
    "    # Mock implementation; in a real case, scrape or access paper content based on the URL\n",
    "    return f\"Extracted data from paper at {paper_url}.\"\n",
    "\n",
    "@tool\n",
    "def summarize_findings(data: str) -> str:\n",
    "    \"\"\"Summarizes findings from the extracted data.\"\"\"\n",
    "    return f\"### Summary of Findings\\n\\n{data}\\n\\n---\\n\"\n",
    "\n",
    "# Enhanced state management\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: Optional[str]\n",
    "    error: Optional[str]\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    steps: List[str] = Field(description=\"Numbered unique steps to follow, in order\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    response: str = Field(description=\"Response to user.\")\n",
    "\n",
    "class Act(BaseModel):\n",
    "    action: Union[Response, Plan] = Field(description=\"Action to perform\")\n",
    "\n",
    "# Improved system prompts\n",
    "SYSTEM_PROMPT = \"\"\"YYou are a research assistant. \n",
    "Your task is to assist with research by gathering and summarizing information on a given topic ONLY using the available tools.\n",
    "\n",
    "Always identify the topic from the input and use it consistently across all steps.\n",
    "\n",
    "Available tools:\n",
    "1. search_research_papers - Searches for research papers on a given topic.\n",
    "2. extract_data - Extracts relevant data from a research paper's URL.\n",
    "3. summarize_findings - Summarizes the findings based on extracted data.\n",
    "\n",
    "Each response should be formatted in Markdown for readability.\"\"\"\n",
    "\n",
    "# Enhanced agent setup\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "tools = [search_research_papers, extract_data, summarize_findings]\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "agent_executor = create_react_agent(llm, tools, state_modifier=prompt)\n",
    "\n",
    "# Improved planning step\n",
    "async def plan_step(state: PlanExecute) -> dict:\n",
    "    try:\n",
    "        planner_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", SYSTEM_PROMPT),\n",
    "            (\"placeholder\", \"{messages}\")\n",
    "        ])\n",
    "        planner = planner_prompt | llm.with_structured_output(Plan)\n",
    "        plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "        return {\"plan\": plan.steps}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Planning error: {str(e)}\"}\n",
    "\n",
    "# Improved execution step with error handling\n",
    "async def execute_step(state: PlanExecute) -> dict:\n",
    "    try:\n",
    "        if \"error\" in state:\n",
    "            return {\"response\": f\"Workflow failed: {state['error']}\"}\n",
    "        \n",
    "        plan = state[\"plan\"]\n",
    "        if not plan:\n",
    "            return {\"response\": \"No plan steps available to execute\"}\n",
    "            \n",
    "        task = plan[0]\n",
    "        agent_response = await agent_executor.ainvoke({\"messages\": [(\"user\", task)]})\n",
    "        return {\"past_steps\": [(task, agent_response[\"messages\"][-1].content)]}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Execution error: {str(e)}\"}\n",
    "\n",
    "# Enhanced replanning with better error handling\n",
    "async def replan_step(state: PlanExecute) -> dict:\n",
    "    try:\n",
    "        if \"error\" in state:\n",
    "            return {\"response\": f\"Workflow failed: {state['error']}\"}\n",
    "            \n",
    "        replanner_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            Given the objective: {input}\n",
    "            Original plan: {plan}\n",
    "            Completed steps: {past_steps}\n",
    "            \n",
    "            Please either:\n",
    "            1. Provide next steps if more work is needed\n",
    "            2. Provide a final response if the workflow is complete\n",
    "            \n",
    "            Only include steps that still need to be done.\n",
    "            \"\"\")\n",
    "        \n",
    "        replanner = replanner_prompt | llm.with_structured_output(Act)\n",
    "        output = await replanner.ainvoke(state)\n",
    "        \n",
    "        if isinstance(output.action, Response):\n",
    "            return {\"response\": output.action.response}\n",
    "        return {\"plan\": output.action.steps}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Replanning error: {str(e)}\"}\n",
    "\n",
    "# Setup workflow\n",
    "def create_workflow():\n",
    "    workflow = StateGraph(PlanExecute)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"planner\", plan_step)\n",
    "    workflow.add_node(\"agent\", execute_step)\n",
    "    workflow.add_node(\"replan\", replan_step)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.add_edge(START, \"planner\")\n",
    "    workflow.add_edge(\"planner\", \"agent\")\n",
    "    workflow.add_edge(\"agent\", \"replan\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"replan\",\n",
    "        lambda s: END if (\"response\" in s or \"error\" in s) else \"agent\",\n",
    "        [END]\n",
    "    )\n",
    "    \n",
    "    return workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "async def run_workflow():\n",
    "    app = create_workflow()\n",
    "    config = {\n",
    "        \"configurable\": {\"thread_id\": \"1\"},\n",
    "        \"recursion_limit\": 50\n",
    "    }\n",
    "    \n",
    "    inputs = {\"input\": f\"LangGraph\"}\n",
    "    \n",
    "    try:\n",
    "        async for event in app.astream(inputs, config=config, stream_mode=\"values\"):\n",
    "            if \"error\" in event:\n",
    "                print(f\"Error: {event['error']}\")\n",
    "                break\n",
    "            print(event)\n",
    "    except Exception as e:\n",
    "        print(f\"Workflow execution failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_workflow())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
