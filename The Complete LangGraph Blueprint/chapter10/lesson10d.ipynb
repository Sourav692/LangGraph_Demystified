{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_core.tools import tool\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, List, Tuple, Union, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "@tool\n",
    "def identify_product(issue: str)->str:\n",
    "    \"\"\"Identifies the product based on the issue description.\"\"\"\n",
    "    if not issue or not isinstance(issue, str):\n",
    "        return \"Error: No issue provided\"\n",
    "    return f\"Identified product related to {issue}.\"\n",
    "\n",
    "@tool\n",
    "def search_manual(product: str, issue: str) -> str:\n",
    "    \"\"\"Searches the product manual for troubleshooting steps.\"\"\"\n",
    "    if not product or not issue or not isinstance(product, str) or not isinstance(issue, str):\n",
    "        return \"Error: Invalid product or issue provided\"\n",
    "    return f\"Searched manual for {product} issue: {issue}. Suggested steps: ...\"\n",
    "\n",
    "@tool\n",
    "def escalate_to_support(product: str, issue: str) ->str:\n",
    "    \"\"\"Escalates the issue to a human support team.\"\"\"\n",
    "    if not product or not issue or not isinstance(product, str) or not isinstance(issue, str):\n",
    "        return \"Error: Invalid product or issue provided\"\n",
    "    return f\"Escalated {product} issue: {issue} to support.\"\n",
    "\n",
    "tools = [identify_product, search_manual, escalate_to_support]\n",
    "\n",
    "# Improved system prompts\n",
    "SYSTEM_PROMPT = \"\"\"You are an customer support assistant. Your task is to help customers troubleshoot product issues \n",
    "using the available tools. Always identify the issue and product from the input and use it consistently across all steps.\n",
    "Available tools:\n",
    "1. identify_product - Identifies the product based on the input.\n",
    "2. search_manual - Searches the product manual for troubleshooting steps\n",
    "3. escalate_to_support - Escalates the issue to a human support team\n",
    "\n",
    "Ensure each step is completed before moving to the next one.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "agent_executor = create_react_agent(llm, tools, state_modifier=prompt)\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str\n",
    "    error: Optional[str]\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    steps: List[str] = Field(description=\"Numbered unique steps to follow, in order\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    response: str = Field(description=\"Response to user.\")\n",
    "\n",
    "class Act(BaseModel):\n",
    "    action: Union[Response, Plan] = Field(description=\"Action to perform\")\n",
    "\n",
    "# Planning step\n",
    "async def plan_step(state: PlanExecute) -> dict:\n",
    "    try:\n",
    "        planner_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", SYSTEM_PROMPT),\n",
    "            (\"placeholder\", \"{messages}\")\n",
    "        ])\n",
    "        planner = planner_prompt | llm.with_structured_output(Plan)\n",
    "        plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "        return {\"plan\": plan.steps}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Planning error: {str(e)}\"}\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    try:\n",
    "        if \"error\" in state:\n",
    "            return {\"response\": f\"Workflow failed: {state['error']}\"}\n",
    "        \n",
    "        plan = state[\"plan\"]\n",
    "        if not plan:\n",
    "            return {\"response\": \"No plan steps available to execute\"}\n",
    "            \n",
    "        task = plan[0]\n",
    "        agent_response = await agent_executor.ainvoke({\"messages\": [(\"user\", task + \" \" + state[\"input\"])]})\n",
    "        return {\"past_steps\": [(task, agent_response[\"messages\"][-1].content)]}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Execution error: {str(e)}\"}\n",
    "    \n",
    "# Enhanced replanning with better error handling\n",
    "async def replan_step(state: PlanExecute) -> dict:\n",
    "    try:\n",
    "        if \"error\" in state:\n",
    "            return {\"response\": f\"Workflow failed: {state['error']}\"}\n",
    "            \n",
    "        replanner_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "            Given the objective: {input}\n",
    "            Original plan: {plan}\n",
    "            Completed steps: {past_steps}\n",
    "            \n",
    "            Please either:\n",
    "            1. Provide next steps if more work is needed\n",
    "            2. Provide a final response if the workflow is complete\n",
    "            \n",
    "            Only include steps that still need to be done.\n",
    "            \"\"\")\n",
    "        \n",
    "        replanner = replanner_prompt | llm.with_structured_output(Act)\n",
    "        output = await replanner.ainvoke(state)\n",
    "        \n",
    "        if isinstance(output.action, Response):\n",
    "            return {\"response\": output.action.response}\n",
    "        return {\"plan\": output.action.steps}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Replanning error: {str(e)}\"}\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    lambda s: END if (\"response\" in s or \"error\" in s) else \"agent\",\n",
    "    [END]\n",
    ")\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}, \"recursion_limit\": 50}\n",
    "\n",
    "async def run_plan_and_execute():\n",
    "    inputs = {\"input\": \"Help troubleshoot my smartphone issue.\"}\n",
    "    async for event in app.astream(inputs, config=config):\n",
    "        print(event)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_plan_and_execute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
